import streamlit as st
from ultralytics import YOLO
from PIL import Image
import numpy as np
import cv2
import tempfile

# –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
st.set_page_config(
    page_title="–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø–æ–º–∏–¥–æ—Ä–æ–≤",
    page_icon="üçÖ",
    layout="wide"
)

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ CSS —Å—Ç–∏–ª–µ–π
st.markdown("""
<style>
    .main-header { font-size: 3rem; color: #ff4b4b; text-align: center; }
    .subheader { font-size: 1.5rem; color: #ff6b6b; }
    .stButton>button { background-color: #ff4b4b; color: white; }
    .stSlider>div>div>div { background-color: #ff4b4b; }
</style>
""", unsafe_allow_html=True)

# –ó–∞–≥–æ–ª–æ–≤–æ–∫
st.markdown('<h1 class="main-header">üçÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø–æ–º–∏–¥–æ—Ä–æ–≤</h1>', unsafe_allow_html=True)
st.markdown("### –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∏ –ø–æ–¥—Å—á–µ—Ç –ø–æ–º–∏–¥–æ—Ä–æ–≤ —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–∏ YOLO")

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
@st.cache_resource
def load_model():
    try:
        model = YOLO("./best.pt")
        return model
    except:
        st.error("–§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –º–æ–¥–µ–ª–∏.")
        return None

model = load_model()

# –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
st.sidebar.header("–ù–∞—Å—Ç—Ä–æ–π–∫–∏")
conf_threshold = st.sidebar.slider("–ü–æ—Ä–æ–≥ –¥–æ–≤–µ—Ä–∏—è", 0.0, 1.0, 0.5, 0.01)
iou_threshold = st.sidebar.slider("–ü–æ—Ä–æ–≥ IOU", 0.0, 1.0, 0.6, 0.01)

# –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç ‚Äî –≤–∫–ª–∞–¥–∫–∏
tab1, tab2, tab3 = st.tabs(["üì∑ –ó–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", "üé• –ó–∞–≥—Ä—É–∑–∏—Ç—å –≤–∏–¥–µ–æ", "üìπ –ö–∞–º–µ—Ä–∞"])

# Tab 1: Image
with tab1:
    uploaded_image = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ–º–∏–¥–æ—Ä–æ–≤", type=['jpg', 'jpeg', 'png'])
    if uploaded_image is not None and model is not None:
        image = Image.open(uploaded_image).convert('RGB')
        col1, col2 = st.columns(2)
        with col1:
            st.image(image, caption="–ó–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", use_column_width=True)
        with st.spinner("–ú–æ–¥–µ–ª—å –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ..."):
            results = model.predict(np.array(image), conf=conf_threshold, iou=iou_threshold)
            result = results[0]
            annotated_image = result.plot()
            annotated_image = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)
        with col2:
            st.image(annotated_image, caption="–û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –ø–æ–º–∏–¥–æ—Ä—ã", use_column_width=True)
            detections = len(result.boxes)
            st.success(f"‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {detections} –ø–æ–º–∏–¥–æ—Ä–æ–≤!")
            if detections > 0:
                with st.expander("–ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"):
                    for i, box in enumerate(result.boxes):
                        conf = box.conf[0].item()
                        cls = result.names[box.cls[0].item()]
                        st.write(f"{i+1}. {cls} - –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {conf:.2f}")

# Tab 2: Video
with tab2:
    uploaded_video = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ –≤–∏–¥–µ–æ —Å –ø–æ–º–∏–¥–æ—Ä–∞–º–∏", type=['mp4', 'mov', 'avi'])
    if uploaded_video is not None and model is not None:
        tfile = tempfile.NamedTemporaryFile(delete=False)
        tfile.write(uploaded_video.read())
        st.info("–í–∏–¥–µ–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è... –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è")
        results = model.predict(tfile.name, conf=conf_threshold, iou=iou_threshold, save=True, project=".", name="output", exist_ok=True)
        try:
            video_results = list(results)[0]
            output_video_path = video_results.save_dir
            st.success("–í–∏–¥–µ–æ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ!")
            st.video("output/output_video.mp4")
        except:
            st.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∏–¥–µ–æ")

# Tab 3: Camera
with tab3:
    st.info("–ö–∞–º–µ—Ä–∞ –≤–∫–ª—é—á–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ –≤–∞—à –±—Ä–∞—É–∑–µ—Ä")
    run_camera = st.checkbox("–ó–∞–ø—É—Å—Ç–∏—Ç—å –∫–∞–º–µ—Ä—É")
    FRAME_WINDOW = st.image([])
    COORDS_WINDOW = st.empty()  # Koordinatalar uchun joy
    if run_camera and model is not None:
        cap = cv2.VideoCapture(0)
        while run_camera:
            ret, frame = cap.read()
            if not ret:
                st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–∞–¥—Ä —Å –∫–∞–º–µ—Ä—ã")
                break
            
            # Agar frame 4-kanal bo'lsa RGB ga o'tkazish
            if frame.shape[2] == 4:
                frame = cv2.cvtColor(frame, cv2.COLOR_BGRA2BGR)
            
            results = model.predict(frame, conf=conf_threshold, iou=iou_threshold)
            result = results[0]
            annotated_frame = result.plot()
            annotated_frame = cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB)
            FRAME_WINDOW.image(annotated_frame)

            # Pomidor koordinatalarini chiqarish
            coords_text = ""
            for i, box in enumerate(result.boxes):
                x1, y1, x2, y2 = box.xyxy[0].tolist()
                coords_text += f"{i+1}. x1={int(x1)}, y1={int(y1)}, x2={int(x2)}, y2={int(y2)}\n"
            if coords_text:
                COORDS_WINDOW.text(f"üìå –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –ø–æ–º–∏–¥–æ—Ä–æ–≤:\n{coords_text}")
            else:
                COORDS_WINDOW.text("üìå –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –ø–æ–º–∏–¥–æ—Ä–æ–≤: –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
        cap.release()
